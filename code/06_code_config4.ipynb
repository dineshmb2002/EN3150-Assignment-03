{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, \n",
    "                            f1_score, precision_score, recall_score, \n",
    "                            accuracy_score, roc_auc_score, cohen_kappa_score,\n",
    "                            matthews_corrcoef)\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------\n",
    "# 0. Device\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load data\n",
    "# ---------------------------\n",
    "image_array = np.load(\"images.npy\", allow_pickle=True)\n",
    "labels = np.load(\"labels.npy\", allow_pickle=True)\n",
    "\n",
    "# Load class names\n",
    "class_names = np.load(\"class_names.npy\", allow_pickle=True)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Labels are already encoded\n",
    "y = labels.astype(np.int64)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Load pre-computed splits\n",
    "train_idx = np.load(\"split_train.npy\")\n",
    "val_idx = np.load(\"split_val.npy\")\n",
    "test_idx = np.load(\"split_test.npy\")\n",
    "\n",
    "print(f\"Train size: {len(train_idx)}, Val size: {len(val_idx)}, Test size: {len(test_idx)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Dataset class\n",
    "# ---------------------------\n",
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, images, labels, indices, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        img = self.images[i].astype(np.uint8)\n",
    "        label = int(self.labels[i])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Model Architecture (Your BetterCNN - UNCHANGED)\n",
    "# ---------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.skip(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class BetterCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
    "        self.layer3 = ResidualBlock(128, 256, stride=2)\n",
    "        self.layer4 = ResidualBlock(256, 512, stride=2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512*4*4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Label Smoothing Loss\n",
    "# ---------------------------\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon=0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        n_class = output.size(1)\n",
    "        log_preds = F.log_softmax(output, dim=1)\n",
    "        loss = -log_preds.sum(dim=1).mean()\n",
    "        nll = F.nll_loss(log_preds, target, weight=self.weight)\n",
    "        return (1 - self.epsilon) * nll + self.epsilon * loss / n_class\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Training Function\n",
    "# ---------------------------\n",
    "def train_with_config(config, image_array, y, train_idx, val_idx, test_idx, class_names, device):\n",
    "    \"\"\"Train BetterCNN model with given hyperparameters\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training Configuration: {config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for key, value in config.items():\n",
    "        if key != 'name':\n",
    "            print(f\"  {key:20s}: {value}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=config['hflip_prob']),\n",
    "        transforms.RandomVerticalFlip(p=config['vflip_prob']),\n",
    "        transforms.RandomRotation(config['rotation_degrees']),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ColorJitter(brightness=config['color_jitter'], \n",
    "                              contrast=config['color_jitter'], \n",
    "                              saturation=config['color_jitter']*0.7),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=config['random_erasing'])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = WasteDataset(image_array, y, train_idx, transform=train_transform)\n",
    "    val_dataset = WasteDataset(image_array, y, val_idx, transform=val_transform)\n",
    "    test_dataset = WasteDataset(image_array, y, test_idx, transform=val_transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n",
    "                             shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n",
    "                           num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], \n",
    "                            num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # Create model\n",
    "    model = BetterCNN(num_classes=num_classes, input_channels=3).to(device)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    # Class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(y[train_idx]),\n",
    "        y=y[train_idx]\n",
    "    )\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = LabelSmoothingCrossEntropy(epsilon=config['label_smoothing'], weight=class_weights)\n",
    "    \n",
    "    # Optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], \n",
    "                              weight_decay=config['weight_decay'])\n",
    "    elif config['optimizer'] == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], \n",
    "                               weight_decay=config['weight_decay'])\n",
    "    else:  # SGD\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], \n",
    "                             momentum=0.9, weight_decay=config['weight_decay'])\n",
    "    \n",
    "    # Scheduler\n",
    "    if config['scheduler'] == 'ReduceLROnPlateau':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7\n",
    "        )\n",
    "    elif config['scheduler'] == 'CosineAnnealing':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=config['epochs'], eta_min=1e-6\n",
    "        )\n",
    "    else:  # OneCycleLR\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=config['learning_rate']*3, \n",
    "            epochs=config['epochs'], steps_per_epoch=len(train_loader)\n",
    "        )\n",
    "    \n",
    "    # Training\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Train]\", leave=False)\n",
    "        for X_batch, y_batch in train_loop:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if config['scheduler'] == 'OneCycleLR':\n",
    "                scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "            \n",
    "            train_loop.set_postfix({'loss': running_loss/total, 'acc': correct/total})\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Val]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loop:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "                \n",
    "                val_loop.set_postfix({'loss': val_loss/total, 'acc': correct/total})\n",
    "        \n",
    "        val_loss = val_loss / total\n",
    "        val_acc = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "        \n",
    "        # Moving average for stability\n",
    "        avg_val_acc = np.mean(val_acc_history[-3:]) if len(val_acc_history) >= 3 else val_acc\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{config['epochs']} | \"\n",
    "              f\"Train: Loss={train_loss:.4f} Acc={train_acc:.4f} | \"\n",
    "              f\"Val: Loss={val_loss:.4f} Acc={val_acc:.4f} AvgAcc={avg_val_acc:.4f}\")\n",
    "        \n",
    "        # Step scheduler\n",
    "        if config['scheduler'] == 'ReduceLROnPlateau':\n",
    "            scheduler.step(val_acc)\n",
    "        elif config['scheduler'] == 'CosineAnnealing':\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_acc > best_val_acc:\n",
    "            best_val_acc = avg_val_acc\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Test on best validation model\n",
    "            model.eval()\n",
    "            correct, total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in test_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    correct += (predicted == y_batch).sum().item()\n",
    "                    total += y_batch.size(0)\n",
    "            best_test_acc = correct / total\n",
    "            \n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_acc': val_acc,\n",
    "                'test_acc': best_test_acc\n",
    "            }, f\"best_model_{config['name']}.pth\")\n",
    "            \n",
    "            print(f\"  ✓ Best model saved! Val={avg_val_acc:.4f}, Test={best_test_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config['patience']:\n",
    "                print(f\"  ✗ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    training_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    # Final test evaluation\n",
    "    checkpoint = torch.load(f\"best_model_{config['name']}.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    y_true, y_pred, y_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_probs = np.array(y_probs)\n",
    "    \n",
    "    final_test_acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_acc = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = y_true == i\n",
    "        if class_mask.sum() > 0:\n",
    "            class_acc = np.mean(y_pred[class_mask] == i)\n",
    "            per_class_acc[str(class_name)] = float(class_acc)\n",
    "    \n",
    "    results = {\n",
    "        'config_name': config['name'],\n",
    "        'config': config,\n",
    "        'best_val_acc': float(best_val_acc),\n",
    "        'final_test_acc': float(final_test_acc),\n",
    "        'per_class_acc': per_class_acc,\n",
    "        'training_time_min': float(training_time),\n",
    "        'epochs_trained': len(train_losses),\n",
    "        'history': {\n",
    "            'train_loss': [float(x) for x in train_losses],\n",
    "            'train_acc': [float(x) for x in train_accs],\n",
    "            'val_loss': [float(x) for x in val_losses],\n",
    "            'val_acc': [float(x) for x in val_accs]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS for {config['name']}:\")\n",
    "    print(f\"  Best Val Acc:  {best_val_acc*100:.2f}%\")\n",
    "    print(f\"  Final Test Acc: {final_test_acc*100:.2f}%\")\n",
    "    print(f\"  Training Time:  {training_time:.1f} minutes\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results, model, y_true, y_pred, y_probs\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Configuration 4 ONLY\n",
    "# ---------------------------\n",
    "config_4 = {\n",
    "    'name': 'config_4_less_augment',\n",
    "    'learning_rate': 0.0005,\n",
    "    'batch_size': 24,\n",
    "    'optimizer': 'AdamW',\n",
    "    'scheduler': 'CosineAnnealing',\n",
    "    'weight_decay': 5e-4,\n",
    "    'label_smoothing': 0.1,\n",
    "    'epochs': 20,\n",
    "    'patience': 12,\n",
    "    'hflip_prob': 0.5,\n",
    "    'vflip_prob': 0.2,\n",
    "    'rotation_degrees': 15,\n",
    "    'color_jitter': 0.2,\n",
    "    'random_erasing': 0.1\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Train Model with Configuration 4\n",
    "# ---------------------------\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(f\"# TRAINING WITH CONFIGURATION 4 ONLY\")\n",
    "print(f\"# Model: BetterCNN (Your Original Architecture)\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "results, model, y_true, y_pred, y_probs = train_with_config(\n",
    "    config_4, image_array, y, train_idx, val_idx, test_idx, class_names, device\n",
    ")\n",
    "\n",
    "# Save results\n",
    "with open(f\"results_{config_4['name']}.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"✓ Saved results to results_{config_4['name']}.json\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. COMPREHENSIVE EVALUATION METRICS\n",
    "# ---------------------------\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(f\"# COMPREHENSIVE EVALUATION METRICS\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "# Calculate all metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "# Print overall metrics\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL METRICS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Accuracy:              {accuracy*100:.2f}%\")\n",
    "print(f\"\\nPrecision (Macro):     {precision_macro*100:.2f}%\")\n",
    "print(f\"Precision (Weighted):  {precision_weighted*100:.2f}%\")\n",
    "print(f\"\\nRecall (Macro):        {recall_macro*100:.2f}%\")\n",
    "print(f\"Recall (Weighted):     {recall_weighted*100:.2f}%\")\n",
    "print(f\"\\nF1-Score (Macro):      {f1_macro*100:.2f}%\")\n",
    "print(f\"F1-Score (Weighted):   {f1_weighted*100:.2f}%\")\n",
    "print(f\"\\nCohen's Kappa:         {kappa:.4f}\")\n",
    "print(f\"Matthews Corr Coef:    {mcc:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Print per-class metrics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PER-CLASS METRICS:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n{classification_report(y_true, y_pred, target_names=class_names, digits=4)}\")\n",
    "\n",
    "# Calculate and print per-class precision, recall, and F1\n",
    "precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DETAILED PER-CLASS BREAKDOWN:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Class':<20} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support'}\")\n",
    "print(\"-\"*80)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    support = np.sum(y_true == i)\n",
    "    print(f\"{class_name:<20} {precision_per_class[i]*100:>10.2f}% \"\n",
    "          f\"{recall_per_class[i]*100:>10.2f}% \"\n",
    "          f\"{f1_per_class[i]*100:>10.2f}% \"\n",
    "          f\"{support:>10}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ---------------------------\n",
    "# 9. CONFUSION MATRIX VISUALIZATION\n",
    "# ---------------------------\n",
    "print(\"\\n\\nGenerating Confusion Matrix...\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot absolute confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, \n",
    "            yticklabels=class_names, ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Confusion Matrix (Absolute Counts)', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[0].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens', xticklabels=class_names, \n",
    "            yticklabels=class_names, ax=axes[1], cbar_kws={'label': 'Proportion'}, \n",
    "            vmin=0, vmax=1)\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[1].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_config4.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Confusion matrix saved to confusion_matrix_config4.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 10. TRAINING HISTORY VISUALIZATION\n",
    "# ---------------------------\n",
    "print(\"\\nGenerating Training History Plot...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "epochs = range(1, len(results['history']['train_acc']) + 1)\n",
    "\n",
    "# Accuracy plot\n",
    "ax = axes[0]\n",
    "ax.plot(epochs, [x*100 for x in results['history']['train_acc']], \n",
    "        label='Train Acc', linewidth=2, marker='o', markersize=6)\n",
    "ax.plot(epochs, [x*100 for x in results['history']['val_acc']], \n",
    "        label='Val Acc', linewidth=2, marker='s', markersize=6)\n",
    "ax.axhline(70, color='red', linestyle='--', linewidth=2, label='Target: 70%')\n",
    "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax = axes[1]\n",
    "ax.plot(epochs, results['history']['train_loss'], \n",
    "        label='Train Loss', linewidth=2, marker='o', markersize=6, color='coral')\n",
    "ax.plot(epochs, results['history']['val_loss'], \n",
    "        label='Val Loss', linewidth=2, marker='s', markersize=6, color='orange')\n",
    "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history_config4.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Training history saved to training_history_config4.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 11. PER-CLASS PERFORMANCE BAR CHART\n",
    "# ---------------------------\n",
    "print(\"\\nGenerating Per-Class Performance Chart...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, precision_per_class * 100, width, label='Precision', \n",
    "               color='steelblue', edgecolor='black')\n",
    "bars2 = ax.bar(x, recall_per_class * 100, width, label='Recall', \n",
    "               color='coral', edgecolor='black')\n",
    "bars3 = ax.bar(x + width, f1_per_class * 100, width, label='F1-Score', \n",
    "               color='lightgreen', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class Precision, Recall, and F1-Score', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.axhline(70, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_metrics_config4.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Per-class metrics chart saved to per_class_metrics_config4.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 12. SUMMARY REPORT\n",
    "# ---------------------------\n",
    "print(f\"\\n\\n{'#'*80}\")\n",
    "print(\"# FINAL SUMMARY REPORT - CONFIGURATION 4\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "print(\"Configuration Details:\")\n",
    "print(\"-\" * 80)\n",
    "for key, value in config_4.items():\n",
    "    if key != 'name':\n",
    "        print(f\"  {key:20s}: {value}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PERFORMANCE SUMMARY:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Test Accuracy:         {accuracy*100:.2f}%\")\n",
    "print(f\"F1-Score (Macro):      {f1_macro*100:.2f}%\")\n",
    "print(f\"F1-Score (Weighted):   {f1_weighted*100:.2f}%\")\n",
    "print(f\"Precision (Macro):     {precision_macro*100:.2f}%\")\n",
    "print(f\"Recall (Macro):        {recall_macro*100:.2f}%\")\n",
    "print(f\"Cohen's Kappa:         {kappa:.4f}\")\n",
    "print(f\"Matthews Corr Coef:    {mcc:.4f}\")\n",
    "print(f\"Training Time:         {results['training_time_min']:.1f} minutes\")\n",
    "print(f\"Epochs Trained:        {results['epochs_trained']}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(\"# EVALUATION COMPLETE!\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "print(\"Generated Files:\")\n",
    "print(f\"  ✓ results_{config_4['name']}.json\")\n",
    "print(f\"  ✓ confusion_matrix_config4.png\")\n",
    "print(f\"  ✓ training_history_config4.png\")\n",
    "print(f\"  ✓ per_class_metrics_config4.png\")\n",
    "print(f\"  ✓ best_model_{config_4['name']}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
